#ml #magistrale 
# Problemi di regressione
Come già detto in [[Definizione di machine learning, supervised learning e algoritmo nearest neighbour#^965cea|precedenza]] nel machine learning i modelli possono essere addestrati per apprendere uno o più parametri. Questo genere di modelli sono detti modelli parametrici, formalizzati come $f(x;\theta)$. Tuttavia una volta che i parametri sono stati appresi, i dati di training possono essere messi da parte dato che la predizione dipende soltanto dal parametro $\theta$ (o da più parametri).  Per la regressione, come visto si hanno etichette che sono numeri reali $y^(i)\in R$ ma in generale i modelli possono essere anche multidimensionali per cui si ha che una etichetta è definita come $y^(i)\in R^m$. Inoltre ricordiamo che il compito della regressione è apprendere la relazione tra alcune variabili di input $x=[x_1,x_2,...x_p]^T$ e una variabile numerica $y$ di output. Tali input possono essere categorici o numerici, ma come detto studieremo il caso in cui gli input sono valori reali. Assumendo che la funzione $f_{true}$ modelli la "vera" relazione tra input e output, dal punto di vista matematico la regressione è apprendere il modello per cui $y=f_{true}+\epsilon$ dove $\epsilon$ è una variabile aleatoria indipendete da $x$ che rappresenta il rumore. Quindi idealmente il nostro modello parametrico è $f(x;\theta)\sim f_{true}(x)$. 

Definiamo adesso una funzione lineare per le feature: $\displaystyle y=f(x)=\sum_{j}w_jx_j+b=w^Tx+b$ dove $y$ è la predizione, $w\in R^d$ è il vettore dei pesi e $b\in R$ è il termine di bias o l'intercetta. In particolare $w\text{ e } b$ sono parametri del modello che andremo collettivamente ad indicare come $\theta$. Se abbiamo solo una feature allora la funzione è quella della rette di regressione $y=wx+b$ ![[regression.png]]
mentre se si hanno $d$ feature siamo nel caso di regressione multipla ![[mult_reg.png|300x300]]
Guardando meglio il grafico della regressione lineare, si nota come in base al valore dei pesi e del bias il risultato cambia sensibilmente. A questo punto ci si chiede, come fare a stabile quanto il modello rappresenta i dati.

## Loss e cost function 
La <font color=orange>loss function</font> o funzione di perdita $\mathcal{L}(y,t)$ definisce dato input, quanto la previsione del modello $y$ è vicina al valore corretto $t$. Intuitivamente, più alta è la loss function, più grande sarà l'errore e se $\mathcal{L}(y,t)=0$ allora $y=t$. Per la regressione, la loss function più usata è la <font color=purple>squared error loss function</font>: $\mathcal{L}(y,t)=(y-t)^2$ dove $(y-t)$ è il *residuo* che si vuole minimizzare. La loss function viene calcolata su una singola predizione, ma noi vogliamo valutare il modello su tutto il training set. A tale scopo definiamo la <font color=blue>cost function</font> o funzione di costo $\mathcal{J}(w,b)$ come la perdita media sul training set. In particolare è pari a: $\displaystyle\mathcal{J}(w,b)={1\over n}\sum^{n}_{i=1}\mathcal{L}(y,t)={1\over n}\sum^{n}_{i=1}(y^{(i)}-t^{(i)})^2={1\over n}\sum^{n}_{i=1}(w^Tx^{(i)}+b-t^{(i)})^2$. Dunque il training avrà come obiettivo determinare parametri ottimali per minimizzare la funzione di costo. Durante il corso potrebbe capitare che si faccia un abuso di terminologia del termine "loss" al posto di "cost".

## Notazioni
Possiamo organizzare tutte le istanze di training all'interndo di una **design matrix** $X=\begin{bmatrix}x^{(1)T} \\ x^{(2)T} \\ x^{(3)T} \end{bmatrix}$ dove una colonna rappresenta i valori di una feature tra tutte le istanze di training mentre una riga rappresenta una sequenza dei valori degli input all'i-esima osservazione.  Ad esempio per la matrice $\begin{bmatrix}0 && 3 && 2 \\ -1 && 5 && 3 \\ 3 && -2 && 8 \end{bmatrix}$ la prima colonna rappresenta tutti i valori possibili di $x_1$ nelle 3 osservazioni fatte, mentre la seconda riga rappresenta i valori di $x_1,x_2,x_3$ nella seconda osservazione. Per quanto riguarda le etichette useremo il vettore $t$. Per calcolare la predizione dell'intero dataset quindi avremo: $Xw+b\mathbf{1}=\begin{pmatrix}w^Tx^{(1)}+b \\ \dots \\ w^Tx^{(n)}+b\end{pmatrix}=\begin{pmatrix}y^{(1)} \\ \dots \\ y^{(n)}\end{pmatrix}=y$ mentre calcolare lo squared error cost è pari a $\displaystyle \mathcal{J}={1\over n}||y-t||^2_2$. Possiamo anche aggiungere una colonna di uno alla design matrix e combinare bias e pesi per scrivere in maniera conveniente $X=\begin{bmatrix}1 && x^{(1)T} \\ 1 && x^{(2)T} \\ 1 && x^{(3)T} \end{bmatrix}\in R^{N\times(d+1)}$ e $w'=\begin{bmatrix} b \\ w_1 \\ \dots \\ w_N \end{bmatrix}\in R^{d+1}$
riducendo cosi il modello a $y=Xw'$ omettendo il termine di bias.

---
# Ottimizzazione della funzione di costo e maximum likelihood estimation
Definito il nostro modello parametrico, come detto l'obiettivo è minimizzare la funzione di costo ovvero trovare i pesi ottimali $\displaystyle w^*=arg\ min_w{1\over n}||Xw-t||^2_2$, e tale funzione di costo è detta <font color=red>least-squares cost function</font>. Gli approcci impiegati sono due: il primo algebrico e il secondo basato sull'analisi attraverso il calcolo del minimo di una funzione "liscia". Tali soluzioni possono essere dirette, se andiamo a trovare parametri ottimali attraverso magari equazioni in forma chiusa, oppure indirette se le tecniche di ottimizzazione si avvicinano iterativamente alla soluzione. Per la least-squares cost function siamo nel caso in cui le soluzioni sono dirette sia nel caso algebrico sia nel caso analitico.
Per quanto riguarda l'approccio algebrico, si utilizza l'algebra lineare per risolvere un problema in cui: date $c_1,c_2,\dots,c_{d+1}$ colonne di $X$, $range(X)=span(c_1,c_2,\dots,c_{d+1})=\{Xw|w\in R^{d+1}\}$ è un sottospazio di $R^n$, dunque scegliere i parametri $w$ equivale a scegliere un vettore in tale sottospazio. Questo ricordando che dato un vettore $t\in R^n$, il vettore più vicino nel sottospazio $range(X)$ si trova tramite la proiezione ortogonale in quel sottospazio. ![[reg_sol_al.png|300x300]] 

Sia $U=range(X)\subseteq R^n$ come mai la proiezione ortogonale $y^*$ è il vettore di $U$ più vicino a $t$? Considerato un arbitrario $z=Xw\in U$ per qualche $w\in R^{d+1}$, allora: $||t-z||^2=||t-y^*+y^*-z||^2=||(y^*-z)+(t-y^*)||^2=||u-e||^2$ dove $y^*-z=u\in U, t-y^*=e \perp U$ e dato che $u\perp e$ possiamo applicare il teorema di Pitagora dunque, $||t-z||^2=||u+e||^2=||u||^2+||e||^2$. Dato che $||u||^2\ge 0$ allora $||t-z||^2\ge||e||^2=||t-y^*||^2$ dunque $||t-z||^2$ è minimo se $u=0$ ovvero $z=Xw=y^*$. Appurato ciò come trovare $w^*$ tale che $Xw^*=y^*$? Abbiamo che $(y^*-t)\perp Xw\ \forall w\in R^{d+1}$, in modo equivalente le colonne di $X$ sono ortogonali a $(y^*-t)$ dunque si ha che: $X^T(y^*-t)=0\implies X^TXw*-X^Tt=0\implies X^TXw=X^Tt$. Questa equazione è nota come <font color=red>normal equation</font>. Abbiamo così ottenuto una soluzione in forma chiusa al problema di ottimizzazione della funzione least-squares e assumendo che $X^TX$ sia invertibile e di solito lo è, si ha che $w^*=(X^TXw)^{-1}X^Tt$

Se invece utilizziamo un approccio basato sull'analisi abbiamo come obiettivo sempre minimizzare la funzione di costo $\mathcal{J}(w)=||Xw-t||^2$, quindi vuol dire trovare i punti critici dove $\nabla_w\mathcal{J}=0$. Ricordando che se la matrice hessiana è definita positiva allora $\mathcal{J}$ è convessa ed abbiamo garantito che un punto critico si tratta di un punto di minimo globale. Ricordando che il gradiente è il vettore delle derivate parziali rispetto a $w$, e che punta nella direzione di maggior incremento della funzione $\mathcal{J}$, porre $\nabla_w\mathcal{J}(w)=0$, significa risolvere un sistema di equazioni. Se esplicitiamo la funzione di costo: $\mathcal{J}(w)=||Xw-t||^2=(Xw-t)^T(Xw-t)=w^TX^TXw+t^Tt-2t^TXw$ e ne calcoliamo il gradiente rispetto $w$ otteniamo che $\nabla_w\mathcal{J}(w)=2X^TXw-2X^Tt=0$ ovvero isolando $w$: $w^*=(X^TXw)^{-1}X^Tt$ proprio il risultato trovato in precedenza. Queste formulazioni hanno dietro una forte motivazione del perché portino ad una soluzione in forma chiusa. Guardando le cose da un punto di vista probabilistico, ci stiamo chiedendo quale parametro $\theta$ rende i nostri dati più probabili da osservare? In altre parole stiamo stimando dei parametri $\theta$ da osservazioni indipendenti identicamente distribuiti cioè $x_1,\dots x_n\sim p(x;\theta)$. 

Definiamo la <font color=orange>funzione di verosimiglianza</font> (likelihood function): $\displaystyle L(\theta)=\prod^{n}_{i=1}p(x_i;\theta)$ e siccome siamo in presenza di indipendenza e vogliamo semplificare i calcoli andremo a massimizzare la *log-likelihood* invece massimizzare $L(\theta)$, ovvero $\displaystyle log\ L(\theta)=\sum^{n}_{i}log\ p(x_i;\theta)$. Si chiama <font color=red>Maximum Likelihood Estimation</font> (MLE) il valore di $\theta$ che massimizza la log-likelihood: $\hat\theta_{MLE}=\arg\ \underset{\theta}{max}\ log\ L(\theta)$, che per dati provenienti da una distribuzione Gaussiana è pari a $\displaystyle \theta=\overline{x}={1\over x}\sum^{n}_{i=1}x_i$ cioè pari alla media. Ritornando alla regressione, come supposto all'inizio $t=f_{true}+\epsilon\implies t=Xw+\epsilon$ dove $\large\epsilon_i\sim \mathcal{N}(0,\sigma_{\epsilon}^2)$ e che i termini del rumore siano indipendenti tra loro, allora le osservazioni in $t$ saranno anche indipendenti perciò avremo che ![[mle.png]] Dunque massimizzare la verosimiglianza equivale a minimizzare la somma degli errori quadratici quando la funzione di costo si dimostra essere l'opposto della MLE!

___
# Regressione non lineare e regolarizzazione
Cosa succede però se la relazione tra input e output non è lineare? Che ovviamente la regressione lineare non è utilizzabile però magari, una relazione lineare esiste se non consideriamo le feature originali, cioè se mappiamo le feature su un altro spazio, può darsi che la relazione sia lineare. Dunque l'idea è di effettuare un mapping: $\psi(x):R^d\rightarrow R^p$ per utilizzare le feature mappate come input per la regressione lineare. Questo stratagemma prende il nome di <font color=orange>feature mapping</font>. Il feature mapping più popolare è la regressione polinomiale, che serve ad adattare i dati tramite una funzione polinomiale di grado $M$ nella forma: $\displaystyle y=w_0+w_1x+w_2x^2+\dots+w_Mx^M=\sum^{M}_{i=0}w_ix^i$. Dunque qui il feature mapping è $\psi(x)=[1,x,x^2,\dots,X^M]^T$ rendendo quindi l'espressione non lineare in $x$, lineare in $\psi$: $\displaystyle y=\sum^{M}_{i=0}w_i\psi_i(x)=w^T\psi(x)$. In questo modo possiamo determinare $w$ attraverso la regressione lineare, andando ad organizzare le istanze di training in una design matrix $\psi(X)=\begin{bmatrix}\psi(x^{(1)})^T \\ \psi(x^{(2)})^T \\ \dots \\ \psi(x^{(n)})^T \end{bmatrix}$ riottenendo $y=\psi(X)w$, $\mathcal{J}(w)={1\over n}||y-t||^2$ e $w^*=(\psi(X)^T\psi(X))^{-1}\psi(X)^Tt$. Attenzione però alla scelta di M:
```media-slider
![[polregunder.png]]
![[polreg.png]]
![[polregwrong.png]]
```
in base al suo valore si avrà underfitting (M=1), overfitting (M=9) oppure una generalizzazione sufficiente (M=3). Si capisce dunque che $M$ è un iperparametro che forse vincola troppo aggressivamente la complessità del modello. Perciò un altro approccio che si può utilizzare è la *regolarizzazione*, un collezione di tecniche che hanno come obiettivo ridurre l'overfitting o comunque migliorare la generalizzazione del modello. Una tecnica semplice ed efficace ad esempio, è aggiungere un termine per regolarizzare la funzione di costo, nello specifico $L_2\ norm$: $\displaystyle \mathcal{R}(w)=||w||^2=\sum_{j}w_j^2$. La risultante funzione di costo sarà: $\displaystyle \mathcal{J}_{reg}(w)=\mathcal{J}(w)+\lambda\mathcal{R}(w)=\mathcal{J}(w)+\lambda\sum_{j}w_j^2$ dove $\lambda$ è un iperparametro. Questa particolare regolarizzazione è detta <font color=red>ridge regression</font> in cui anche in questo caso, possiamo andare a risolvere un problema di minimo per avere $w^*=(X^TX+\lambda I)^{-1}X^Tt$

---
# Regressione logistica